# S3 to RDS Pipeline

## Introduction

This project implements a simple S3 to RDS pipeline that uses an AWS Lambda function triggered by new objects in an S3 bucket. The pipeline can process **CSV, JSON, and JSONL** files and store the data in a PostgreSQL database hosted on RDS.

## Pipeline Design

The pipeline was built using **AWS Cloud** services. The architecture is as follows:

### Data Generation and Ingress

Data is ingested into S3 using a Python script with **boto3**. The data is generated by `create_random_data.py` and consists of random vehicle ID data for BMW cars. The data generation uses the **Faker** library, and some entries are intentionally error-prone to facilitate **data cleaning and validation**.

### Lambda Function

When data lands in S3, it triggers an AWS Lambda function. Two versions of the function were developed:

* **Naive Approach**: The entire data file is loaded into memory using **pandas**, processed, and then inserted into PostgreSQL.
* **Batch Approach**: The file is read in chunks using **pandas**, and the same processing and insertion logic is applied to each chunk.

#### Purpose

The two approaches allow us to **stress test the function** and explore **cost optimization**. The naive approach requires more memory and resources (~500 MB RAM), while the batch approach reduces resource usage by 50%, albeit with a slight increase in execution time (~40 seconds per run).

### PostgreSQL

The processed data is stored in a **PostgreSQL** database. A table `car_data` is created to store all the final data. Partitioning is applied to improve **query efficiency** for searches by `model` or `city`:

```sql
CREATE TABLE IF NOT EXISTS public.car_data
(
    ...
)
PARTITION BY HASH(model, city);

CREATE TABLE IF NOT EXISTS public.car_data_part1 
PARTITION OF public.car_data 
FOR VALUES WITH (MODULUS 4, REMAINDER 0);
...
```

The full table creation script is available in `scripts/create_table.sql`.

## Pipeline Setup and Running

1. **Install Prerequisites**

   * Download and install **Terraform** and the **AWS CLI**.

2. **Configure AWS Credentials**

```bash
export AWS_ACCESS_KEY_ID=<your-access-key>
export AWS_SECRET_ACCESS_KEY=<your-secret-key>
```

3. **Verify Credentials**

```bash
aws configure list
```

4. **Deploy Infrastructure**

   * Navigate to the `terraform/` folder.

```bash
terraform init
terraform apply
```

* When prompted, type **yes**.

5. **Add Lambda Layers**

   * In the AWS Console, navigate to **Lambda → Layers**. Add the **psycopg2** layer from `lambda/layers/psycopg-layer-p312`.
   * Go to your Lambda function **S3ToPG**, click **Layers → Add layer**, and add the ARN for the psycopg2 layer. Also add **AWSSDKPandas-Python312**.

6. **Configure Environment Variables**
   In the Lambda function GUI, set the following variables:

```text
RDS_URL: "URL of your PostgreSQL database"
DB_NAME: postgres
USER_NAME: postgresadmin
PASSWORD: YourPassword123!
```

Once these steps are complete, the pipeline is ready to run.

## Cleanup

To destroy all resources after use:

* **Using Script**

```bash
sh ./clean.sh
```

* **Manually**

```bash
# Remove data from bucket
aws s3 rm s3://s3-to-pg-bucket-dev --recursive

# Destroy Terraform infrastructure
terraform destroy
```

---
